{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from helpers import get_llm, get_retriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough, RunnableParallel, RunnableLambda, chain\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated\n",
    "import requests\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions and Schema\n",
    "In langchain, schema can be defined together with the actual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"get_weather\")\n",
    "def get_weather(\n",
    "    location: Annotated[str, \"Location for the weather forecast, e.g. London, UK\"]\n",
    "):\n",
    "    \"\"\"Forecast the weather for the provided location.\"\"\"\n",
    "\n",
    "    api_key = \"777c42660156447db5842748240110\"\n",
    "    result = requests.get(\n",
    "        f\"https://api.weatherapi.com/v1/current.json?key={api_key}&q={location}\"\n",
    "    )\n",
    "\n",
    "    return result.json()\n",
    "\n",
    "@tool(\"get_interest_rate\")\n",
    "def get_interest_rate(\n",
    "    amount: Annotated[int, \"Amount of deposit\"],\n",
    "    interest_rate: Annotated[float, \"Interest rate percentage\"],\n",
    "    term: Annotated[int, \"Maturity period in month\"],\n",
    "):\n",
    "    \"\"\"Interest calculation for fixed deposit.\"\"\"\n",
    "    \n",
    "    interest = amount * (interest_rate / 100) * (term / 12)\n",
    "    if term > 36:\n",
    "        interest += 111\n",
    "\n",
    "    return interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tool Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather, get_interest_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = get_retriever(\n",
    "    index_name=\"test\",\n",
    "    embedding_model=\"text-embedding-3-large\",\n",
    "    dimension=256,\n",
    "    vector_db=\"qdrant\",\n",
    "    top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm(\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Instructions for RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_instructions = \"\"\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"\"\"\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_instructions),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_question = contextualize_prompt | llm | StrOutputParser()\n",
    "\n",
    "qa_instructions = (\n",
    "    \"\"\"Use tool calls if necessary. Answer the user question given the following context:\\n\\n{context}.\"\"\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", qa_instructions), (\"human\", \"{question}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a RAG Chain Combined with Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def tool_call(input_: dict) -> Runnable:\n",
    "    llm_result = input_.get(\"llm_result\")\n",
    "    print(llm_result)\n",
    "    if llm_result.tool_calls:\n",
    "        test_instruction = \"\"\"Answer the question using the tool response.\"\"\"\n",
    "        test_prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", test_instruction), (\"human\", \"{question}\")]\n",
    "        )\n",
    "        test_prompt.messages.append(llm_result)\n",
    "        for tool_call in llm_result.tool_calls:\n",
    "            selected_tool = next(\n",
    "                temp_tool for temp_tool in tools if temp_tool.name == tool_call[\"name\"]\n",
    "            )\n",
    "            tool_response = selected_tool.invoke(tool_call)\n",
    "            test_prompt.messages.append(tool_response)\n",
    "        return {\"question\": itemgetter(\"question\")} | test_prompt | llm_with_tools\n",
    "\n",
    "    else:\n",
    "        return llm_result\n",
    "\n",
    "@chain\n",
    "def contextualize_if_needed(input_: dict) -> Runnable:\n",
    "    if input_.get(\"chat_history\"):\n",
    "        return contextualize_question\n",
    "    else:\n",
    "        return RunnablePassthrough() | itemgetter(\"question\")\n",
    "\n",
    "# Pass input query to retriever\n",
    "retrieve_docs_chain = itemgetter(\"question\") | retriever\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\".join(doc.page_content for doc in docs)\n",
    "\n",
    "formatted_prompt = {\n",
    "        \"question\": itemgetter(\"question\") | RunnablePassthrough(),\n",
    "        \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "    } | RunnableParallel(prompt=qa_prompt, question=itemgetter('question'))\n",
    "\n",
    "\n",
    "llm_result_chain = formatted_prompt | RunnableParallel(llm_result=itemgetter('prompt') | llm_with_tools, question=itemgetter(\"question\"))\n",
    "\n",
    "output_chain = llm_result_chain | tool_call | StrOutputParser()\n",
    "\n",
    "final_chain = (\n",
    "    RunnablePassthrough.assign(question=contextualize_if_needed)\n",
    "    .assign(context=retrieve_docs_chain)\n",
    "    .assign(answer=output_chain)\n",
    ")\n",
    "\n",
    "# final_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"How much interest can i get if I put in 50000RM for 48 months in fixed deposit account?\"\n",
    "question = \"How is the weather today in Yangon?\"\n",
    "result = final_chain.invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    "    , config={\"callbacks\": [ConsoleCallbackHandler()]}\n",
    ")\n",
    "chat_history = []\n",
    "chat_history.append((\"human\", question))\n",
    "chat_history.append((\"ai\", result[\"answer\"]))\n",
    "context = result[\"context\"]\n",
    "chat_history, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Potential Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm(model_name=\"meta-llama/Meta-Llama-3-70B-Instruct\", temperature=0.9)\n",
    "prompt = \"\"\"Generate {k} possible follow up questions based on the given chat history and context. You should follow the instructions below.\n",
    "\n",
    "- Don't answer the questions.\n",
    "- Don't include any introductory text, explanations, or follow-up sentences.\n",
    "- Don't number the question list.\n",
    "- Keep the questions short and direct. \n",
    "- Only generate contextually answerable questions.\n",
    "- List the questions in a single line, separated by commas without whitespaces.\n",
    "\n",
    "Example response: What is a cat?,How many legs do they have?\n",
    "---------------------------------------------------\n",
    "Chat History: {chat_history}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "def get_chat_history(input_):\n",
    "  return \"\\n\".join([f\"{role}: {content}\" for role, content in input_])\n",
    "\n",
    "def get_context(input_):\n",
    "  return input_[0].page_content\n",
    "\n",
    "result = {\n",
    "  \"chat_history\": itemgetter(\"chat_history\") | RunnableLambda(get_chat_history), \n",
    "  \"context\": itemgetter('context') | RunnableLambda(get_context),\n",
    "  \"k\": itemgetter('k')\n",
    "} | prompt_template | llm | StrOutputParser()\n",
    "result.invoke({\n",
    "  \"chat_history\": chat_history,\n",
    "  \"context\": context,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
