{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test\",\n",
    "    client=chroma_client,\n",
    "    embedding_function=embedding,\n",
    ")\n",
    "\n",
    "top_k = 5\n",
    "retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": top_k},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.reranker_integration import get_reranker\n",
    "\n",
    "reranker = get_reranker(base_retriever=retriever, model_name=\"BAAI/bge-reranker-base\", top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.llm_integrations import get_llm\n",
    "\n",
    "llm = get_llm(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Create Contextualize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    chain,\n",
    ")\n",
    "from operator import itemgetter\n",
    "\n",
    "contextualize_instructions = \"\"\"Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text).\"\"\"\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "      [\n",
    "          (\"system\", contextualize_instructions),\n",
    "          (\"placeholder\", \"{chat_history}\"),\n",
    "          (\"human\", \"{question}\"),\n",
    "      ]\n",
    "  )\n",
    "contextualize_question = contextualize_prompt | llm | StrOutputParser()\n",
    "\n",
    "@chain\n",
    "def contextualize_if_needed(input_: dict) -> Runnable:\n",
    "    if input_.get(\"chat_history\"):\n",
    "        return contextualize_question\n",
    "    else:\n",
    "        return RunnablePassthrough() | itemgetter(\"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Answer the questions using the given context.\"\n",
    "\n",
    "qa_instructions = instruction + \"\"\"\\n\\n{context}.\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "  [(\"system\", qa_instructions), (\"human\", \"{question}\")]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\".join(doc.page_content for doc in docs)\n",
    "\n",
    "formatted_prompt = {\n",
    "    \"question\": itemgetter(\"question\") | RunnablePassthrough(),\n",
    "    \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "} | RunnableParallel(prompt=qa_prompt, question=itemgetter(\"question\"))\n",
    "\n",
    "qa_chain = formatted_prompt | RunnableParallel(\n",
    "    llm_result=itemgetter(\"prompt\") | llm,\n",
    "    question=itemgetter(\"question\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Retrieval Chain (Pass Reranker Instead of Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs_chain = itemgetter(\"question\") | reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Usage Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LLMResultHandler(BaseCallbackHandler):\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        if response.generations[0][0].message.usage_metadata:\n",
    "            token_usage = response.generations[0][0].message.usage_metadata\n",
    "        else:\n",
    "            usage = response.generations[0][0].message.response_metadata[\"token_usage\"]\n",
    "            token_usage = {\n",
    "                \"input_tokens\": usage.prompt_tokens,\n",
    "                \"output_tokens\": usage.completion_tokens,\n",
    "                \"total_tokens\": usage.total_tokens,\n",
    "            }\n",
    "        self.response = token_usage\n",
    "\n",
    "llm_result_handler = LLMResultHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langfuse Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "langfuse_args = {}\n",
    "langfuse_handler = (\n",
    "        CallbackHandler(\n",
    "          **langfuse_args\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Final Chain (Contextualize -> Retrieval -> Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = (\n",
    "        RunnablePassthrough.assign(question=contextualize_if_needed)\n",
    "        .assign(context=retrieve_docs_chain)\n",
    "        .assign(answer=qa_chain)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"What is fixed deposit?\"\n",
    "result = final_chain.invoke(\n",
    "        {\"question\": input, \"chat_history\": []},\n",
    "        config={\n",
    "            \"callbacks\": [llm_result_handler, langfuse_handler]\n",
    "        },\n",
    "    )\n",
    "\n",
    "answer = result[\"answer\"]\n",
    "source_documents = [\n",
    "    {\"page_content\": doc.page_content, \"source\": doc.metadata[\"source\"]}\n",
    "    for doc in result[\"context\"]\n",
    "]\n",
    "\n",
    "token_usage = llm_result_handler.response\n",
    "\n",
    "output = {\n",
    "    \"answer\": answer,\n",
    "    \"source_documents\": source_documents,\n",
    "    \"token_usage\": token_usage,\n",
    "}\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
